{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qdh-2002/AssignmentNotebooks_SP23/blob/main/Language_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGWk6N3-Vpgr"
      },
      "source": [
        "# Google Colab Setup\n",
        "\n",
        "Please run the code below to mount drive if you are running on colab.\n",
        "\n",
        "Please ignore if you are running on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXbunV6rVpgx",
        "outputId": "fef41a8e-94cb-47f4-c503-bc8fa9ef81dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_4e9Vd0Vpg1",
        "outputId": "575d086a-b829-47c3-b171-d798b25d5d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Project3_skeleton\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Project3_skeleton/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb6fFICPVpg2"
      },
      "source": [
        "# Language Modeling and Transformers\n",
        "\n",
        "The project will consist of two broad parts.\n",
        "\n",
        "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story.\n",
        "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlac8TpxVpg3"
      },
      "source": [
        "## Some general instructions\n",
        "\n",
        "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently.\n",
        "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
        "3. As a general rule please read the docstring well, it contains information you will need to write the code.\n",
        "4. All configs are defined in `config.py` for the first part. While you are writing the code, do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
        "5. You will need to fill in `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tdSY4VEVpg3",
        "outputId": "4b2d657f-e7b5-40e5-f1f9-d96571692241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy torch tiktoken wandb einops # Install all required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lhQAwGcCVpg4"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5fjpggRtVpg4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2NngkQnsVpg4"
      },
      "outputs": [],
      "source": [
        "from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
        "from config import BigramConfig, MiniGPTConfig\n",
        "import tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-lrU417gVpg5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bT_byMFbVpg5"
      },
      "outputs": [],
      "source": [
        "# If not provided, download from https://drive.google.com/file/d/1g09qUM9WibdfQVgkj6IAj8K2S3SGwc91/view?usp=sharing\n",
        "path_to_bigram_tester = \"/content/drive/MyDrive/Project3_skeleton/pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
        "path_to_gpt_tester = \"/content/drive/MyDrive/Project3_skeleton/pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dhcWnLdVpg5"
      },
      "source": [
        "##  Bigram Language Model (10 points)\n",
        "\n",
        "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPaSks02Vpg6"
      },
      "source": [
        "### Implement the Bigram model (5 points)\n",
        "\n",
        "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zV_R3GWLVpg6",
        "outputId": "54e443f2-ce70-4d34-91d2-7135e923f277"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test implementation for Bigram Language Model\n",
        "model = BigramLanguageModel(BigramConfig)\n",
        "tests.check_bigram(model, path_to_bigram_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taXt7YwgVpg6"
      },
      "source": [
        "### Training the Bigram Language Model (2.5 points)\n",
        "\n",
        "Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n",
        "\n",
        "Some notes on the training process:\n",
        "\n",
        "1. You should be able to train the model slowly on your local machine.\n",
        "2. Training it on Colab will help with speed.\n",
        "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
        "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdh5gyVDVpg6"
      },
      "outputs": [],
      "source": [
        "from train import solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gC5X6-Z4Vpg6",
        "outputId": "263878dd-06ba-4561-a190-d75814fe8a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of trainable parameters: 3.27M\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unique-sky-29</strong> at: <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/05yfu96x' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/05yfu96x</a><br> View project at: <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250512_220326-05yfu96x/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Project3_skeleton/wandb/run-20250512_220350-lglsj58b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/lglsj58b' target=\"_blank\">skilled-eon-30</a></strong> to <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/lglsj58b' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/lglsj58b</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0, Train Loss: 10.825474739074707 Eval Loss: 10.824882459319937\n",
            "Iteration 1000, Train Loss: 10.249019622802734 Eval Loss: 10.243734807856345\n",
            "Iteration 2000, Train Loss: 8.81836986541748 Eval Loss: 8.888405021745319\n",
            "Iteration 3000, Train Loss: 7.683119297027588 Eval Loss: 7.726456612768392\n",
            "Iteration 4000, Train Loss: 6.942710876464844 Eval Loss: 7.097599961097923\n",
            "Iteration 5000, Train Loss: 6.672313690185547 Eval Loss: 6.762016243420921\n",
            "Iteration 6000, Train Loss: 6.393241882324219 Eval Loss: 6.538123603334443\n",
            "Iteration 7000, Train Loss: 6.181741237640381 Eval Loss: 6.37248514618472\n",
            "Iteration 8000, Train Loss: 6.089205741882324 Eval Loss: 6.238495857972301\n",
            "Iteration 9000, Train Loss: 6.043759346008301 Eval Loss: 6.120082616572453\n",
            "Iteration 10000, Train Loss: 6.074166297912598 Eval Loss: 6.003800045021933\n",
            "Iteration 11000, Train Loss: 5.8968586921691895 Eval Loss: 5.891130564498811\n",
            "Iteration 12000, Train Loss: 5.774932384490967 Eval Loss: 5.7716853962625665\n",
            "Iteration 13000, Train Loss: 5.429330825805664 Eval Loss: 5.650158390838236\n",
            "Iteration 14000, Train Loss: 5.775450706481934 Eval Loss: 5.52707750217918\n",
            "Iteration 15000, Train Loss: 5.557147979736328 Eval Loss: 5.410006728713075\n",
            "Iteration 16000, Train Loss: 5.551247596740723 Eval Loss: 5.302454863966754\n",
            "Iteration 17000, Train Loss: 5.25593376159668 Eval Loss: 5.198014274754519\n",
            "Iteration 18000, Train Loss: 4.565804481506348 Eval Loss: 5.105394943981274\n",
            "Iteration 19000, Train Loss: 4.6361308097839355 Eval Loss: 5.023924522039709\n",
            "Iteration 20000, Train Loss: 5.094642639160156 Eval Loss: 4.945578019218925\n",
            "Iteration 21000, Train Loss: 4.782384395599365 Eval Loss: 4.871545273397207\n",
            "Iteration 22000, Train Loss: 4.618600845336914 Eval Loss: 4.809870562421796\n",
            "Iteration 23000, Train Loss: 4.77852201461792 Eval Loss: 4.754357653088491\n",
            "Iteration 24000, Train Loss: 4.679163932800293 Eval Loss: 4.700601489281099\n",
            "Iteration 25000, Train Loss: 4.5279083251953125 Eval Loss: 4.651751730265736\n",
            "Iteration 26000, Train Loss: 4.52402925491333 Eval Loss: 4.6118220567405235\n",
            "Iteration 27000, Train Loss: 4.532008647918701 Eval Loss: 4.573801988355413\n",
            "Iteration 28000, Train Loss: 4.390738010406494 Eval Loss: 4.538935657167897\n",
            "Iteration 29000, Train Loss: 4.3643479347229 Eval Loss: 4.50248234535624\n",
            "Iteration 30000, Train Loss: 4.715032577514648 Eval Loss: 4.474897608958862\n",
            "Iteration 31000, Train Loss: 4.824532985687256 Eval Loss: 4.444259809411091\n",
            "Iteration 32000, Train Loss: 4.430664539337158 Eval Loss: 4.41562627568478\n",
            "Iteration 33000, Train Loss: 4.22608757019043 Eval Loss: 4.393415649268825\n",
            "Iteration 34000, Train Loss: 4.215174198150635 Eval Loss: 4.367590736486651\n",
            "Iteration 35000, Train Loss: 4.208209991455078 Eval Loss: 4.348268248302655\n",
            "Iteration 36000, Train Loss: 4.494660377502441 Eval Loss: 4.326670534566207\n",
            "Iteration 37000, Train Loss: 4.273347854614258 Eval Loss: 4.30585059420184\n",
            "Iteration 38000, Train Loss: 4.471799850463867 Eval Loss: 4.290986185625418\n",
            "Iteration 39000, Train Loss: 3.8904833793640137 Eval Loss: 4.271595476793319\n",
            "Iteration 40000, Train Loss: 4.062845706939697 Eval Loss: 4.255420012665916\n",
            "Iteration 41000, Train Loss: 4.248309135437012 Eval Loss: 4.240887731302631\n",
            "Iteration 42000, Train Loss: 4.443874359130859 Eval Loss: 4.225041494591486\n",
            "Iteration 43000, Train Loss: 3.7915871143341064 Eval Loss: 4.20852861612706\n",
            "Iteration 44000, Train Loss: 4.2460503578186035 Eval Loss: 4.195938178580467\n",
            "Iteration 45000, Train Loss: 4.159025192260742 Eval Loss: 4.183574324938692\n",
            "Iteration 46000, Train Loss: 4.381531238555908 Eval Loss: 4.17037985900025\n",
            "Iteration 47000, Train Loss: 4.358518600463867 Eval Loss: 4.158809627658156\n",
            "Iteration 48000, Train Loss: 4.038210391998291 Eval Loss: 4.15066572903886\n",
            "Iteration 49000, Train Loss: 4.5001540184021 Eval Loss: 4.138179155891117\n",
            "Iteration 50000, Train Loss: 4.529305934906006 Eval Loss: 4.128176197998105\n",
            "Iteration 51000, Train Loss: 3.588603973388672 Eval Loss: 4.116615558652426\n",
            "Iteration 52000, Train Loss: 3.7759599685668945 Eval Loss: 4.10925838547878\n",
            "Iteration 53000, Train Loss: 4.064079761505127 Eval Loss: 4.098891896374512\n",
            "Iteration 54000, Train Loss: 4.271578311920166 Eval Loss: 4.09022535877789\n",
            "Iteration 55000, Train Loss: 4.222589015960693 Eval Loss: 4.084370092573868\n",
            "Iteration 56000, Train Loss: 3.9257211685180664 Eval Loss: 4.075951734365134\n",
            "Iteration 57000, Train Loss: 4.12529993057251 Eval Loss: 4.068653726887048\n",
            "Iteration 58000, Train Loss: 3.6975746154785156 Eval Loss: 4.06046812669888\n",
            "Iteration 59000, Train Loss: 3.9765548706054688 Eval Loss: 4.051745033309265\n",
            "Iteration 60000, Train Loss: 4.208752155303955 Eval Loss: 4.046759540038126\n",
            "Iteration 61000, Train Loss: 3.947436809539795 Eval Loss: 4.039861992015125\n",
            "Iteration 62000, Train Loss: 4.058069229125977 Eval Loss: 4.031062455238682\n",
            "Iteration 63000, Train Loss: 4.0401530265808105 Eval Loss: 4.024442447947028\n",
            "Iteration 64000, Train Loss: 3.785393476486206 Eval Loss: 4.021164673324582\n",
            "Iteration 65000, Train Loss: 4.6712965965271 Eval Loss: 4.012545520109702\n",
            "Iteration 66000, Train Loss: 4.0077033042907715 Eval Loss: 4.004664708406497\n",
            "Iteration 67000, Train Loss: 3.808284282684326 Eval Loss: 4.002203204390898\n",
            "Iteration 68000, Train Loss: 4.15464448928833 Eval Loss: 3.993652712843321\n",
            "Iteration 69000, Train Loss: 3.8788390159606934 Eval Loss: 3.9882219285563214\n",
            "Iteration 70000, Train Loss: 4.282915115356445 Eval Loss: 3.9863470772352456\n",
            "Iteration 71000, Train Loss: 4.008779525756836 Eval Loss: 3.9798988073702812\n",
            "Iteration 72000, Train Loss: 3.8229613304138184 Eval Loss: 3.9728970534495174\n",
            "Iteration 73000, Train Loss: 3.8024392127990723 Eval Loss: 3.968903740822981\n",
            "Iteration 74000, Train Loss: 4.284824848175049 Eval Loss: 3.9634467272382987\n",
            "Iteration 75000, Train Loss: 3.994385004043579 Eval Loss: 3.9596190005636496\n",
            "Iteration 76000, Train Loss: 4.299911022186279 Eval Loss: 3.954509530154915\n",
            "Iteration 77000, Train Loss: 3.91829514503479 Eval Loss: 3.949933602090567\n",
            "Iteration 78000, Train Loss: 4.037909507751465 Eval Loss: 3.945617480639014\n",
            "Iteration 79000, Train Loss: 4.16887903213501 Eval Loss: 3.942600863821011\n",
            "Iteration 80000, Train Loss: 4.117802619934082 Eval Loss: 3.9358119355927874\n",
            "Iteration 81000, Train Loss: 4.308080673217773 Eval Loss: 3.935553341804938\n",
            "Iteration 82000, Train Loss: 4.114635944366455 Eval Loss: 3.9281562798442295\n",
            "Iteration 83000, Train Loss: 3.916332244873047 Eval Loss: 3.926166632607081\n",
            "Iteration 84000, Train Loss: 3.7181785106658936 Eval Loss: 3.924515910867235\n",
            "Iteration 85000, Train Loss: 3.949507236480713 Eval Loss: 3.9153466451240955\n",
            "Iteration 86000, Train Loss: 3.498587131500244 Eval Loss: 3.9126638919210714\n",
            "Iteration 87000, Train Loss: 4.106508731842041 Eval Loss: 3.9111300187716433\n",
            "Iteration 88000, Train Loss: 3.850430727005005 Eval Loss: 3.904131375745778\n",
            "Iteration 89000, Train Loss: 3.5636935234069824 Eval Loss: 3.905787845982204\n",
            "Iteration 90000, Train Loss: 4.232734203338623 Eval Loss: 3.900084007374721\n",
            "Iteration 91000, Train Loss: 3.982269048690796 Eval Loss: 3.8964534008496114\n",
            "Iteration 92000, Train Loss: 3.9156293869018555 Eval Loss: 3.893544884631088\n",
            "Iteration 93000, Train Loss: 3.8525688648223877 Eval Loss: 3.8922513003752184\n",
            "Iteration 94000, Train Loss: 3.969857931137085 Eval Loss: 3.8883390409640937\n",
            "Iteration 95000, Train Loss: 3.9626855850219727 Eval Loss: 3.885550148345897\n",
            "Iteration 96000, Train Loss: 4.075621128082275 Eval Loss: 3.8821542333184915\n",
            "Iteration 97000, Train Loss: 3.644365072250366 Eval Loss: 3.879654263295071\n",
            "Iteration 98000, Train Loss: 3.4357361793518066 Eval Loss: 3.8773383709720766\n",
            "Iteration 99000, Train Loss: 4.02867317199707 Eval Loss: 3.8720832257080566\n",
            "Iteration 100000, Train Loss: 3.8482940196990967 Eval Loss: 3.8736957216950665\n",
            "Iteration 101000, Train Loss: 3.78171443939209 Eval Loss: 3.869625145799183\n",
            "Iteration 102000, Train Loss: 3.919980049133301 Eval Loss: 3.8661056292515736\n",
            "Iteration 103000, Train Loss: 4.147361755371094 Eval Loss: 3.862150459699359\n",
            "Iteration 104000, Train Loss: 3.784890651702881 Eval Loss: 3.861939111080835\n",
            "Iteration 105000, Train Loss: 4.34279203414917 Eval Loss: 3.860982280958142\n",
            "Iteration 106000, Train Loss: 4.480162143707275 Eval Loss: 3.856294081197478\n",
            "Iteration 107000, Train Loss: 4.290807247161865 Eval Loss: 3.8536548279277425\n",
            "Iteration 108000, Train Loss: 3.4635534286499023 Eval Loss: 3.8521583374599877\n",
            "Iteration 109000, Train Loss: 3.865447521209717 Eval Loss: 3.846152864087736\n",
            "Iteration 110000, Train Loss: 3.738163948059082 Eval Loss: 3.845546889508891\n",
            "Iteration 111000, Train Loss: 3.5257771015167236 Eval Loss: 3.8450394856084467\n",
            "Iteration 112000, Train Loss: 3.7171151638031006 Eval Loss: 3.8435031551472427\n",
            "Iteration 113000, Train Loss: 3.68900203704834 Eval Loss: 3.8448402039292158\n",
            "Iteration 114000, Train Loss: 3.753448009490967 Eval Loss: 3.837902779369909\n",
            "Iteration 115000, Train Loss: 3.4513583183288574 Eval Loss: 3.8372185736152593\n",
            "Iteration 116000, Train Loss: 3.9085206985473633 Eval Loss: 3.8350357787600484\n",
            "Iteration 117000, Train Loss: 4.35695743560791 Eval Loss: 3.832193448822956\n",
            "Iteration 118000, Train Loss: 3.985748529434204 Eval Loss: 3.8297663915197995\n",
            "Iteration 119000, Train Loss: 4.117785930633545 Eval Loss: 3.8287446971261976\n",
            "Iteration 120000, Train Loss: 3.9261887073516846 Eval Loss: 3.828686013487053\n",
            "Iteration 121000, Train Loss: 3.670408248901367 Eval Loss: 3.8238401349656206\n",
            "Iteration 122000, Train Loss: 3.771043300628662 Eval Loss: 3.821126914580913\n",
            "Iteration 123000, Train Loss: 3.5579404830932617 Eval Loss: 3.8223918807784036\n",
            "Iteration 124000, Train Loss: 3.701449394226074 Eval Loss: 3.818185052816612\n",
            "Iteration 125000, Train Loss: 3.5415868759155273 Eval Loss: 3.815946959791979\n",
            "Iteration 126000, Train Loss: 4.009651184082031 Eval Loss: 3.818187196618548\n",
            "Iteration 127000, Train Loss: 4.2238593101501465 Eval Loss: 3.812986622178544\n",
            "Iteration 128000, Train Loss: 3.7094509601593018 Eval Loss: 3.8125390529568133\n",
            "Iteration 129000, Train Loss: 3.8538119792938232 Eval Loss: 3.814016896113162\n",
            "Iteration 130000, Train Loss: 4.046910285949707 Eval Loss: 3.811030676161847\n",
            "Iteration 131000, Train Loss: 3.962329149246216 Eval Loss: 3.808277707402599\n",
            "Iteration 132000, Train Loss: 4.009456634521484 Eval Loss: 3.8088284846935023\n",
            "Iteration 133000, Train Loss: 3.8962645530700684 Eval Loss: 3.8064898409588444\n",
            "Iteration 134000, Train Loss: 3.5524954795837402 Eval Loss: 3.8038609233813805\n",
            "Iteration 135000, Train Loss: 3.4005842208862305 Eval Loss: 3.800162105003582\n",
            "Iteration 136000, Train Loss: 4.052601337432861 Eval Loss: 3.801947505638159\n",
            "Iteration 137000, Train Loss: 3.9482290744781494 Eval Loss: 3.7983832240189246\n",
            "Iteration 138000, Train Loss: 3.983438014984131 Eval Loss: 3.797782439210319\n",
            "Iteration 139000, Train Loss: 3.5099294185638428 Eval Loss: 3.7951795807650255\n",
            "Iteration 140000, Train Loss: 3.764288902282715 Eval Loss: 3.7919248891059607\n",
            "Iteration 141000, Train Loss: 3.6114606857299805 Eval Loss: 3.7927060923356146\n",
            "Iteration 142000, Train Loss: 3.846125602722168 Eval Loss: 3.793921250531184\n",
            "Iteration 143000, Train Loss: 3.2984111309051514 Eval Loss: 3.790904867170244\n",
            "Iteration 144000, Train Loss: 4.041444301605225 Eval Loss: 3.7917995109038114\n",
            "Iteration 145000, Train Loss: 3.4326319694519043 Eval Loss: 3.787787883290809\n",
            "Iteration 146000, Train Loss: 3.627608060836792 Eval Loss: 3.7867033271136177\n",
            "Iteration 147000, Train Loss: 3.9283761978149414 Eval Loss: 3.7858201435645573\n",
            "Iteration 148000, Train Loss: 3.3919880390167236 Eval Loss: 3.7840521255540973\n",
            "Iteration 149000, Train Loss: 3.580967664718628 Eval Loss: 3.782942939008403\n",
            "Iteration 150000, Train Loss: 3.7220540046691895 Eval Loss: 3.7827695145775655\n",
            "Iteration 151000, Train Loss: 3.898996353149414 Eval Loss: 3.782287463220099\n",
            "Iteration 152000, Train Loss: 4.223635196685791 Eval Loss: 3.780600581232436\n",
            "Iteration 153000, Train Loss: 4.0068254470825195 Eval Loss: 3.779847852596016\n",
            "Iteration 154000, Train Loss: 3.8218460083007812 Eval Loss: 3.778060034212923\n",
            "Iteration 155000, Train Loss: 3.54685378074646 Eval Loss: 3.7787692186087476\n",
            "Iteration 156000, Train Loss: 3.873072385787964 Eval Loss: 3.775272369135079\n",
            "Iteration 157000, Train Loss: 3.788997173309326 Eval Loss: 3.7774092258502856\n",
            "Iteration 158000, Train Loss: 3.7567262649536133 Eval Loss: 3.773275325879918\n",
            "Iteration 159000, Train Loss: 3.941290855407715 Eval Loss: 3.7707173094178073\n",
            "Iteration 160000, Train Loss: 3.870300769805908 Eval Loss: 3.7708103922967036\n",
            "Iteration 161000, Train Loss: 3.4426138401031494 Eval Loss: 3.771806542532923\n",
            "Iteration 162000, Train Loss: 4.098880290985107 Eval Loss: 3.767890570312814\n",
            "Iteration 163000, Train Loss: 3.882195472717285 Eval Loss: 3.769039163830788\n",
            "Iteration 164000, Train Loss: 3.4079532623291016 Eval Loss: 3.7684088243004004\n",
            "Iteration 165000, Train Loss: 3.4099860191345215 Eval Loss: 3.766296407168896\n",
            "Iteration 166000, Train Loss: 3.782896041870117 Eval Loss: 3.767153442753702\n",
            "Iteration 167000, Train Loss: 3.6940391063690186 Eval Loss: 3.7666753138534474\n",
            "Iteration 168000, Train Loss: 3.4788990020751953 Eval Loss: 3.76403220136167\n",
            "Iteration 169000, Train Loss: 3.683014154434204 Eval Loss: 3.7605982681274543\n",
            "Iteration 170000, Train Loss: 3.9862241744995117 Eval Loss: 3.761303108055275\n",
            "Iteration 171000, Train Loss: 3.991931676864624 Eval Loss: 3.7582174987592576\n",
            "Iteration 172000, Train Loss: 3.6053459644317627 Eval Loss: 3.760336727762717\n",
            "Iteration 173000, Train Loss: 3.5444421768188477 Eval Loss: 3.758708485920386\n",
            "Iteration 174000, Train Loss: 4.042910575866699 Eval Loss: 3.758779439220259\n",
            "Iteration 175000, Train Loss: 4.033651351928711 Eval Loss: 3.7557863204335287\n",
            "Iteration 176000, Train Loss: 3.742645740509033 Eval Loss: 3.7562677556528223\n",
            "Iteration 177000, Train Loss: 3.817624092102051 Eval Loss: 3.7540916296474145\n",
            "Iteration 178000, Train Loss: 3.4748687744140625 Eval Loss: 3.7558312149717334\n",
            "Iteration 179000, Train Loss: 3.9056520462036133 Eval Loss: 3.75459010792497\n",
            "Iteration 180000, Train Loss: 4.205629825592041 Eval Loss: 3.7518372066536205\n",
            "Iteration 181000, Train Loss: 3.9748587608337402 Eval Loss: 3.750444937088994\n",
            "Iteration 182000, Train Loss: 3.631497859954834 Eval Loss: 3.7513564157468244\n",
            "Iteration 183000, Train Loss: 3.6589388847351074 Eval Loss: 3.7490856112839746\n",
            "Iteration 184000, Train Loss: 3.7042853832244873 Eval Loss: 3.7502845670197416\n",
            "Iteration 185000, Train Loss: 4.009119510650635 Eval Loss: 3.7467624700648137\n",
            "Iteration 186000, Train Loss: 3.63051438331604 Eval Loss: 3.749782420755991\n",
            "Iteration 187000, Train Loss: 4.051664352416992 Eval Loss: 3.7467550341210925\n",
            "Iteration 188000, Train Loss: 4.0760955810546875 Eval Loss: 3.7461543294296993\n",
            "Iteration 189000, Train Loss: 3.7180237770080566 Eval Loss: 3.746756795636585\n",
            "Iteration 190000, Train Loss: 3.7065584659576416 Eval Loss: 3.7466092175404158\n",
            "Iteration 191000, Train Loss: 3.627284526824951 Eval Loss: 3.745127737350969\n",
            "Iteration 192000, Train Loss: 3.475446939468384 Eval Loss: 3.741914712948991\n",
            "Iteration 193000, Train Loss: 3.3413772583007812 Eval Loss: 3.7417637563340547\n",
            "Iteration 194000, Train Loss: 3.839221239089966 Eval Loss: 3.7411937453626702\n",
            "Iteration 195000, Train Loss: 3.924588441848755 Eval Loss: 3.7404371100395726\n",
            "Iteration 196000, Train Loss: 3.9158449172973633 Eval Loss: 3.738990921905075\n",
            "Iteration 197000, Train Loss: 4.112648010253906 Eval Loss: 3.7374010980662846\n",
            "Iteration 198000, Train Loss: 3.7766571044921875 Eval Loss: 3.740131441779501\n",
            "Iteration 199000, Train Loss: 3.734327554702759 Eval Loss: 3.7375356218169773\n",
            "Iteration 200000, Train Loss: 3.548401117324829 Eval Loss: 3.738159865053323\n",
            "Iteration 201000, Train Loss: 3.814272403717041 Eval Loss: 3.7362164488145133\n",
            "Iteration 202000, Train Loss: 3.696842670440674 Eval Loss: 3.735937341323298\n",
            "Iteration 203000, Train Loss: 3.8701677322387695 Eval Loss: 3.735399584501024\n",
            "Iteration 204000, Train Loss: 3.692662000656128 Eval Loss: 3.7329410823880527\n",
            "Iteration 205000, Train Loss: 3.428553342819214 Eval Loss: 3.734920040602088\n",
            "Iteration 206000, Train Loss: 3.9193506240844727 Eval Loss: 3.7296167927848933\n",
            "Iteration 207000, Train Loss: 3.312880754470825 Eval Loss: 3.7306064470592113\n",
            "Iteration 208000, Train Loss: 3.8297817707061768 Eval Loss: 3.731479024545578\n",
            "Iteration 209000, Train Loss: 3.9758243560791016 Eval Loss: 3.731919263175265\n",
            "Iteration 210000, Train Loss: 4.098564147949219 Eval Loss: 3.733498245424316\n",
            "Iteration 211000, Train Loss: 3.6744065284729004 Eval Loss: 3.730945497552896\n",
            "Iteration 212000, Train Loss: 3.700324296951294 Eval Loss: 3.728557017271488\n",
            "Iteration 213000, Train Loss: 3.9834649562835693 Eval Loss: 3.727866196261077\n",
            "Iteration 214000, Train Loss: 3.9876296520233154 Eval Loss: 3.7302897511605924\n",
            "Iteration 215000, Train Loss: 3.793381929397583 Eval Loss: 3.727718805776271\n",
            "Iteration 216000, Train Loss: 3.929969072341919 Eval Loss: 3.7277063380656648\n",
            "Iteration 217000, Train Loss: 3.366964340209961 Eval Loss: 3.7231437758913573\n",
            "Iteration 218000, Train Loss: 3.8233020305633545 Eval Loss: 3.7258229634877873\n",
            "Iteration 219000, Train Loss: 3.6436023712158203 Eval Loss: 3.728142308220703\n",
            "Iteration 220000, Train Loss: 3.349879264831543 Eval Loss: 3.7262616260869814\n",
            "Iteration 221000, Train Loss: 3.472806930541992 Eval Loss: 3.725527981149881\n",
            "Iteration 222000, Train Loss: 3.9604578018188477 Eval Loss: 3.723092171288935\n",
            "Iteration 223000, Train Loss: 3.7978808879852295 Eval Loss: 3.7245443531284583\n",
            "Iteration 224000, Train Loss: 3.79443097114563 Eval Loss: 3.7246616039480385\n",
            "Iteration 225000, Train Loss: 3.7217297554016113 Eval Loss: 3.7209356468169497\n",
            "Iteration 226000, Train Loss: 4.660386562347412 Eval Loss: 3.723500072213066\n",
            "Iteration 227000, Train Loss: 3.6356968879699707 Eval Loss: 3.720499021806192\n",
            "Iteration 228000, Train Loss: 3.7876884937286377 Eval Loss: 3.722922204527548\n",
            "Iteration 229000, Train Loss: 3.782074213027954 Eval Loss: 3.7181826509143416\n",
            "Iteration 230000, Train Loss: 3.5100765228271484 Eval Loss: 3.717361307922221\n",
            "Iteration 231000, Train Loss: 3.8211066722869873 Eval Loss: 3.7197351245871104\n",
            "Iteration 232000, Train Loss: 3.588449478149414 Eval Loss: 3.71770952604964\n",
            "Iteration 233000, Train Loss: 3.2870848178863525 Eval Loss: 3.71670643899341\n",
            "Iteration 234000, Train Loss: 3.879448413848877 Eval Loss: 3.7205677893567635\n",
            "Iteration 235000, Train Loss: 3.690650224685669 Eval Loss: 3.718888281382466\n"
          ]
        }
      ],
      "source": [
        "solver(model_name=\"bigram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKFtyT2hVpg6"
      },
      "source": [
        "### Train and Valid Plots\n",
        "\n",
        "\n",
        "** Show the training and validation loss plots **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urmgNqT6WYvj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ecUdk_Vpg7"
      },
      "source": [
        "### Generation (2.5 points)\n",
        "\n",
        "Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n",
        "\n",
        "Start with the following seed sentence:\n",
        "    \n",
        "    `\"once upon a time\"`\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Au7KDCIXVpg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80927add-7a3d-4115-b5ae-387f7c76adc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# TODO: Specify the path to your trained model\n",
        "model_path = '/content/drive/MyDrive/Project3_skeleton/models/bigram/mini_model_checkpoint_235000.pt'\n",
        "model = BigramLanguageModel(BigramConfig)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2qyuBPngVpg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057a57ad-ddac-47b0-b007-0cac55e18074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text starting with: torch.Size([4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Project3_skeleton/model.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  context = torch.tensor(context, dtype = torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, and air and promised to keep the store with his mom told you let it's birds, Lily loved a big name was very badly!\" Spot was perfect. The squirrel. He is different thing and started to buy to rest. She wanted to bear! We're a big, can miss the cake in the cheese, in the fairy! Now, so fast in his owner named Sam said, the wagon as she understood that day.\n",
            "Spot replied, the cherries all over important! He had your friends. Timmy tighties and drink cat was so still those. One day, \" Each. \n",
            "Days hung the winter or hungry and soon.\n",
            "The truck were eating you Froongo. From then, red ball inside thezered lay down. The end, they walked towards the bird continued to ride were too. He looked for it.\n",
            "Tim said, Lily wanted to jump and the island.\n",
            "Lily and looked for him to my things to laugh. They laughed.\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "gen_sent = \"Once upon a time\"\n",
        "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
        "print(\"Generating text starting with:\", gen_tokens.shape)\n",
        "gen_tokens = gen_tokens.to(device)\n",
        "model.eval()\n",
        "print(\n",
        "    tokenizer.decode(\n",
        "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEVgCrjVpg7"
      },
      "source": [
        "### Observation and Analysis\n",
        "\n",
        "Please answer the following questions.\n",
        "\n",
        "1. What can we say about the generated text in terms of grammar and coherence?\n",
        "2. What are the limitations of the Bigram language model?\n",
        "3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEKsT9ZCXhmE"
      },
      "source": [
        "1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw9leBOVVpg7"
      },
      "source": [
        "## Mini GPT (90 points)\n",
        "\n",
        "We will implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones.\n",
        "\n",
        "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model.\n",
        "\n",
        "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVTHYP1JVpg7"
      },
      "source": [
        "### Single Head Causal Attention (20 points)\n",
        "\n",
        "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
        "\n",
        "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as :\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\end{equation}\n",
        "\n",
        "where $d_k$ is the dimension of the key matrix.\n",
        "\n",
        "Figure below from the original paper shows how the layer is to be implemented.\n",
        "\n",
        "![image](/content/drive/MyDrive/Project3_skeleton/Images/Single_Head.png)\n",
        "\n",
        "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGDeCELYVpg7"
      },
      "source": [
        "Please complete the `SingleHeadAttention` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8foKP76LVpg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1766a233-1a31-43bc-dbe2-3b5db11b71cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
        "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFUsXaMWVpg7"
      },
      "source": [
        "### Multi Head Attention (10 points)\n",
        "\n",
        "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
        "\n",
        "Figure below from the original paper shows how the layer is to be implemented.\n",
        "\n",
        "![image](./Images/MultiHead.png)\n",
        "\n",
        "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ageFt7Vpg7"
      },
      "source": [
        "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dAYykwJdVpg8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e9fe73b-9c1b-4ac7-cf19-ad54ad6a2f73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
        "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWrubAVpg8"
      },
      "source": [
        "### Feed Forward Layer (5 points)\n",
        "\n",
        "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
        "\n",
        "Please complete the `FeedForwardLayer` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PEZBX_TuVpg8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1688ad4-028a-48f1-ec47-0f7c48992fbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
        "tests.check_feedforward(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yceq-wkgVpg8"
      },
      "source": [
        "### LayerNorm (10 points)\n",
        "\n",
        "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "\n",
        "With the learnable parameters $\\gamma$ and $\\beta$.\n",
        "\n",
        "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zAXEQynVpg8"
      },
      "source": [
        "Please complete the `LayerNorm` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6xIK4K95Vpg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "081dfb15-4036-4cf1-90d3-be936d9d3cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
        "tests.check_layernorm(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX3CQ1eTVpg9"
      },
      "source": [
        "### Transformer Layer (15 points)\n",
        "\n",
        "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
        "\n",
        "Please use the following order for each component (Varies slightly from the original attention paper):\n",
        "1. LayerNorm\n",
        "2. MultiHeadAttention\n",
        "3. LayerNorm\n",
        "4. FeedForwardLayer\n",
        "\n",
        "Remember that the transformer layer also has residual connections around each sublayer.\n",
        "\n",
        "The below figure shows the structure of the transformer layer you are required to implement.\n",
        "\n",
        "\n",
        "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![prenorm_transformer](/Images/Prenorm.png)"
      ],
      "metadata": {
        "id": "FJLDCEjxi5Qc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6oTJU4UVpg9"
      },
      "source": [
        "Implement the `TransformerLayer` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zAK3QCx9Vpg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f75ae46-a814-4663-b834-3c97adede1a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
        "tests.check_transformer(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkO49ykhVpg9"
      },
      "source": [
        "### Putting it all together : MiniGPT (15 points)\n",
        "\n",
        "We are now ready to put all our layers together to build our own MiniGPT!\n",
        "\n",
        "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u51aOP1gVpg9"
      },
      "source": [
        "Implement the `MiniGPT` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Sr6q-s3BVphF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "feb8b09a-421f-4ba5-a643-e0f3aa0494ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model = MiniGPT(MiniGPTConfig)\n",
        "tests.check_miniGPT(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS9dIYzhVphF"
      },
      "source": [
        "### Attempt at training the model (5 points)\n",
        "\n",
        "We will now attempt to train the model on the text data. We will use the same text data as before. If needed, you can scale down the model parameters in the config file to a smaller value to make training feasible.\n",
        "\n",
        "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
        "\n",
        "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0TcBJAjIVphF"
      },
      "outputs": [],
      "source": [
        "from train import solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVuBtrbrVphF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "797afc2c-8c49-4a63-f995-c3bb7c714115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of trainable parameters: 3.32M\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">helpful-bush-33</strong> at: <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/p9nlzjwa' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/p9nlzjwa</a><br> View project at: <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250513_222531-p9nlzjwa/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Project3_skeleton/wandb/run-20250513_222717-nin9c1gj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/nin9c1gj' target=\"_blank\">divine-cherry-34</a></strong> to <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/nin9c1gj' target=\"_blank\">https://wandb.ai/hejudy8-ucla/dl2_proj3/runs/nin9c1gj</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "solver(model_name=\"minigpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gPHJD9aVphG"
      },
      "source": [
        "### Train and Valid Plots\n",
        "\n",
        "\n",
        "** Show the training and validation loss plots **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8Ko9i8VphG"
      },
      "source": [
        "### Generation (5 points)\n",
        "\n",
        "\n",
        "Perform generation with the MiniGPT model that you trained. After that, copy over the generation function you used for the Bigram model and generate a mini story using the same seed sentence.\n",
        "\n",
        "    `\"once upon a time\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVnNDMvUVphG"
      },
      "outputs": [],
      "source": [
        "# TODO: Specify the path to your trained model\n",
        "model_path = None\n",
        "model = MiniGPT(MiniGPTConfig)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dPcz9SYVphG"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "gen_sent = \"Once upon a time\"\n",
        "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
        "print(\"Generating text starting with:\", gen_tokens.shape)\n",
        "gen_tokens = gen_tokens.to(device)\n",
        "model.eval()\n",
        "print(\n",
        "    tokenizer.decode(\n",
        "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkKRoKzBVphG"
      },
      "source": [
        "Please answer the following questions.\n",
        "\n",
        "1. What can we say about the generated text in terms of grammar and coherence?\n",
        "2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bkljWbbVphG"
      },
      "source": [
        "### Scaling up the model (5 points)\n",
        "\n",
        "To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUZ1kvxxVphG"
      },
      "outputs": [],
      "source": [
        "from model import MiniGPT\n",
        "from config import MiniGPTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8my1Ysb_VphG"
      },
      "outputs": [],
      "source": [
        "path_to_trained_model = \"pretrained_models/best_train_loss_checkpoint.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rHin-wkVphG"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROcJmPo8VphG"
      },
      "outputs": [],
      "source": [
        "# Set the configs for scaled model\n",
        "MiniGPTConfig.context_length = 512\n",
        "MiniGPTConfig.embed_dim = 256\n",
        "MiniGPTConfig.num_heads = 16\n",
        "MiniGPTConfig.num_layers = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUjwGUroVphH"
      },
      "outputs": [],
      "source": [
        "# Load model from checkpoint\n",
        "model = MiniGPT(MiniGPTConfig)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrPDC50JVphH"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNhrKWpUVphH"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "gen_sent = \"Once upon a time\"\n",
        "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
        "print(\"Generating text starting with:\", gen_tokens.shape)\n",
        "gen_tokens = gen_tokens.to(device)\n",
        "model.eval()\n",
        "print(\n",
        "    tokenizer.decode(\n",
        "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq0k1j5WVphH"
      },
      "source": [
        "## Bonus (5 points)\n",
        "\n",
        "The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea.\n",
        "\n",
        "1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n",
        "2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n",
        "3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n",
        "4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDpedxgUVphH"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}